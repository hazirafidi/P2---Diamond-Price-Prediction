# -*- coding: utf-8 -*-
"""Diamond_price_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13IZDWZxZDoQCwVZu1dr6v7Wj3Gx8FMB-

# **Diamonds Price Prediction using Feedforward Neural Network**
"""

# import all necessary modules/libraries 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf

# Load the data
diamonds = pd.read_csv("/content/sample_data/diamonds.csv")

# print the first 5 rows of the data content
diamonds.head()

# Check all the columns and index of the dataset
diamonds.info()

# In this case study, price should be the label as we are trying to predict the price of the diamonds based on the features given in the dataset
# We need to inspect and pre-process the data first
# Let's check wether there is null column in the dataset
print(diamonds.iloc[:,:].isnull().sum())

# Okay, looks like the data is perfect!

# Now we have to remove the unnecessary column that will not be using for the model training
# In this case we are removing the id column
diamonds = diamonds.drop(columns = diamonds.iloc[:,[0]], axis=1)

print(diamonds.head())

# Check the diamonds dataframe
print(diamonds.info())

# Now we have to convert all string into numerical encoding
diamonds = pd.get_dummies(diamonds, prefix = ['cut', 'color', 'clarity'])

print(diamonds.head())

# Check all columns after transformation
print(diamonds.info())

# Now we have to split the data into labels and features
diamonds_features = diamonds.copy().drop(columns='price')
diamonds_labels = diamonds.copy().pop('price')

print(diamonds_features.head())
print("\n")
print(diamonds_labels.head())

# We can convert the labels and features into numpy array
diamonds_features_np = np.array(diamonds_features)
diamonds_labels_np = np.array(diamonds_labels)

print("diamonds_features_shape: ", diamonds_features_np.shape)
print("diamonds_labels_shape: ", diamonds_labels_np.shape)

# Now that the labels and features has splitted, we can perform train_test_split
SEED=1234
X_train, X_test, y_train, y_test = train_test_split(diamonds_features_np, diamonds_labels_np, test_size=0.2, random_state=SEED)

print(f'Size of X_train: "{len(X_train)}"')
print(f'Size of X_test: "{len(X_test)}"')
print(f'Size of y_train: "{len(y_train)}"')
print(f'Size of y_test: "{len(y_test)}"')

# We need to standardize the train and test dataset
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print(X_train[5:,5])
print("\n")
print(X_test[5:,5])

# Now that data preprocessing is done, we can proceed with building the model
# We are going to use functional API model from tensorflow to build a model for training
inputs = tf.keras.Input(shape = X_train.shape[1])
dense = tf.keras.layers.Dense(64, activation='relu')
x = dense(inputs)
dense = tf.keras.layers.Dense(32, activation='relu')
x = dense(x)
dense = tf.keras.layers.Dense(16, activation='relu')
x = dense(x)
dense = tf.keras.layers.Dense(8, activation='relu')
x = dense(x)
outputs = tf.keras.layers.Dense(1, activation='linear')(x)

model = tf.keras.Model(inputs=inputs, outputs=outputs, name= 'diamonds_model')

# Check the built model
model.summary()

# Now we can compile the model if everything is in order
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.summary()

tf.keras.utils.plot_model(model)

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard
from gc import callbacks
import datetime, os
logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)
es_callback = tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 3)

# train the model
BATCH_SIZE = 50
EPOCHS = 100
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=BATCH_SIZE,
                    epochs=EPOCHS, callbacks=[tensorboard_callback, es_callback])

#tf.keras.backend.clear_session()

# Commented out IPython magic to ensure Python compatibility.
# View training accuracy and loss graph via tensorboard
# %tensorboard --logdir logs

# Evaluate the model
model.evaluate(X_test, y_test, verbose=2)

# make prediction with the trained model
print(model.predict(np.expand_dims(X_test[100], axis=0)))

#visualize the error graph
import matplotlib.pyplot as plt

# plot the graph of training loss vs val_loss
training_loss = history.history['loss']
val_loss = history.history['val_loss']
epoch = history.epoch

plt.plot(epoch, training_loss, label = 'Training MSE')
plt.plot(epoch, val_loss, label = 'Validation MSE')
plt.title('Training vs Validation MSE')
plt.legend()
plt.xlabel('epochs')
plt.ylabel('MSE')
plt.show()

print("\n")

#plot the graph of training accuracy vs validation loss
training_accuracy = history.history['mae']
val_accuracy = history.history['val_mae']
epoch = history.epoch

plt.plot(epoch, training_accuracy, label = 'Training MAE')
plt.plot(epoch, val_accuracy, label = 'Validation MAE')
plt.title('Training vs Validation MAE')
plt.legend()
plt.xlabel('epochs')
plt.ylabel('MAE')
plt.show()

# Plot the scatter graph prediction vs label
predictions = np.squeeze(model.predict(X_test))
labels = np.squeeze(y_test)
plt.plot(predictions, labels, "o")
plt.xlabel("Predictions")
plt.ylabel("Labels")
plt.title("Prediction against labels with test data")
plt.show()